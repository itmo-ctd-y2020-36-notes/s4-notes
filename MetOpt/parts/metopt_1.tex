\chapter{О предмете}

Слова оптимизация просисходит от слова \textit{optimus} --- поиск наилучшего решения.

Чем эта дисциплина занимается?

Поиск минимума или максимума какой-либо функции.
$f(x) \to \min (\max)$.

Например, $f(x)$ -- стоимость, которую мы хотим минимизировать.

Обычно мы бдуем рассматривать функции, действующие из множества в числа, $f: A \to \R$.

Поиск решения задачи, где у нас оптимизируется несколько параметро -- многокритериальная оптимизация.

Во многокритериальную оптимизацию сильно углубляться мы не будем.

Методы математической оптимизации также называют математическим программированием (программмирование $\equiv$ поиск оптимального плана).


\section{Какиого вида может быть функция $f$?}

\subsection{Линейная}
Пусть $f(x) = \varphi \cdot x$.

Как правило, есть дополлнительные ограничения $A \cdot x = b$ ($A$ -- матрица, $b$ -- вектор).
$x_i \geqslant 0$.

\begin{example}[Транспортная задача]
    Есть $n$ складов, $m$ магазинов как эффективнее огранизовать логистику?
\end{example}

Вообще, бывают задачи, где ограничения есть и где их нет.

\subsection{Квадратичная функция}
Пусть $f(x) = \varphi \cdot x + x^T \cdot \theta \cdot x$.

Простоейшая задача -- линейная регрессия.
При помощи линейной функции покрыть множество точек, так чтобы сумма квадратов отклонений была минимальным.

Так как квадратичная функция выпукла, то ответ, обычно, бывает один.

В более сложных случаях глобавльных минимумов может не быть и придется искать локальные минимумы.

\subsection{Нельнейная функция}
$f(X) \to \min, f(X) \to \R$.
Иногда, функция может быть дискретной.
Например, в задачах динамического программирования.

\begin{example}
    Пусть есть окружность с радиусом $R$, нужно вписать в него прямоугольник со сторонами $a$ и $b$.

    $f = a \cdot b$.
    Ограничение $\sqrt{a^2 + b^2} = 2R$.
\end{example}

\begin{example}
    Обчуение различных моделей машинного обучения.
\end{example}


\section{Методы решения}
Иногда можно решить аналитически или в явном виде.

Но, это получается отнюдь не всегда.

\begin{definition}
[Итеррационные методы решения]
    Позволяют на каждом своем шаге как-то уточнять результаты решения.
    И таким образом можно получить, возможно, не точное решение, но достаточно близкое к нему.
\end{definition}

Пусть есть некоторая фунция $f: \R^n \to \R$.
Методы решения бывают детерминимрованными и стахастическими.

Один из методов решения -- \textbf{метод Монте-Карло} (генерируем случайные решения, проверяем, что они подходят, выбираем среди них оптимальный).
В этом методе надо вычислять только значние функции.

Еще один метод -- \textbf{метод иммитации отжига}.
Мы находимся в точке, выбираем еще одну точку.
Сморим, там значение меньше или больше в зависимости от этого получаем вроятность, что мы туда перейем.

Классификация методов по порядкам:
\begin{itemize}
    \item Нулевого порядка -- считаем только значние функции;
    \item Первого парядка -- используем градиенты изменения функции;
    \item Второго порядка -- используем вторые производные.
\end{itemize}

Еще один метод нулевого порядка -- \textbf{метод Нелдера -- Мида} (похоже на симпликс-метод, но для большего множества задач).

И еще один метод нулевого порядка -- \textbf{эволюционный метод или генетические алгоритмы}.
Использование подходов монте-карло с методами эволюции -- скрещеванием, матацией и отбором.

Методы \textbf{первого порядка} используют производные или градиенты.
Из математичсекого анализа известно, что градиент показывает направление наибольшего роста фнукции и, следуя туда, можно найти какой-то экстремум.

Самый простейший метод -- градиентный спуск.
Берем точку начального приближения, считаем в ней градиент и шагаем по направлению градиента.
Такой метод сходися.
На некоторых классах функции дает достаточно неплохой отевет.

Метод наискорейшего спуска.
Считаем градиент в одной точке и там, где в том направлении, где градиент минимальный.
Ищем в том направлении минимум и переходим туда.

В методах \textbf{второго порядка} используются матрица вторых производных -- гессиан.
Ингда читать вторые производные достаточно накладно.
Поэтому,  такие методы применяются достаточнно редко и для специфических задач с небольшим количством параметров.

Один из таких методов -- метод ньютона.

Есть \textbf{квази--ньютоновские методы}.
Они приближают матрицу вторых производных и это позволяет не считать матрицу явно.
BFGS.


\section{Почему Python?}
Python на данный момент является стандартом как для научного программирования, так и для, в частности, машинного обучения.

С одной сторны, питон очень простой.
С другой стороны, на питоне есть большое количество библиотек.

Да, Python очень медленный.
Однако есть библиотеки, например numpy, которые используют оптимальные оптимизации и засчет этого будет большой выигрыш.

\subsection{Библиотеки для python, без которых жить будет сложно}

\subsubsection{NumPy} -- самая важная библиотека.
Позволяет работать с многомерными числовыми массивами и выполнять операции над ними.

Зачастую нужно будет сформулировать свой алгоритм так, чтобы он выражался в виде операций над массивами.

Оффициальный сайт \href{https://numpy.org/doc/stable}{numpy.org/doc/stable}.

\textbf{Broadcasting}.
В nympy особенное предобразование массивов.

Если умножить число на массив, число неявно превратится в массив и после этого произойдет поэлементное умножение.
То же самое есть с массивами разных размерностей.

\subsubsection{SciPy}
Библиотека для научных вычислений на Python.
В ней есть много уже готовых алгоритмов, в том числе методов оптимизации.

Сайт \href{https://scipy.github.io}{scipy.github.io}.

\subsubsection{MatPlotLib}
Библиотека для построения различных визуализаций, графиков и т.д.

Сайт \href{https://matplotlib.org/}{matplotlib.org}

\subsubsection{Pandas}
Библиотека для работы с таблицами и табличными данными.

Сайт \href{https://pandas.pydata.org}{pandas.pydata.org}

\subsection{Как работать с python}
Есть такое понятие, на \textbf{jupiter notebook}.
Это интерактивные блокноты в которых можно писать на python или других языках,
писать текст с математическими формулами и делать раличные визулизации.

Редакторы можно использовать различные.
Можно писать в VS Code или PyCharm, еще есть Google Cloud.

\subsubsection{Массивы в python}
\begin{lstlisting}
    a = [1, 2, 3, 4, 5, 6, 7, 8, 9]
    a[-1] # 9
    a[2:2] # [3, 4]
    a[::-1] # [9, 8, 7, 6, 5, 4, 3, 2, 2, 1]
\end{lstlisting}

\subsection{Испольщование numpy}
\begin{lstlisting}
    import numpy as np
    a = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9])
    a * 2 # [2, 4, 6, 8, 10, 12, 14, 16, 18]
    [1, 2, 3] + [4, 5, 6] # [1, 2, 3, 4, 5, 6]
\end{lstlisting}

\begin{lstlisting}
    a = np.arrange(0, 10)
    a # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
    a + 10 # array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10])
    a ** 2 # array([0, 1, 4, 9, 16, 25, 36, 49, 64, 81])
    np.sin(a) # sin foreach elements
    np.sum(a) # sum of elements
    a[a > 3] # [4, 5, 6, 7, 8, 9]
    a.shape() # (10,)
    a.reshape(2, 5) # [[0, 1, 2, 3, 4], [5, 6, 7, 8, 9]]
    a.reshape(2, 5).T # [[0, 5], [1, 6], [2, 7], [3, 8], [4, 9]]
    a.reshape(2, 5) * [-1, 2] # [[0, 2], [2, 6], [4, 10], [6, 14], [8, 18]]

    b = np.array([[0, 1], [2, 3], [4, 5], [6, 7], [8, 9]])
    b[:, 0] # [0, 2, 4, 6, 8] -- first column
    b[0, :] # [0, 1] -- first line
    b[:2, :2] # [[0, 1, [2, 3]]

    np.array([1, 2, 3, 4, 5])[:, np.newaxis]
\end{lstlisting}

Графики
\begin{lstlisting}
    x = n.linspace(0, 10, 100)
    plt.plot(x, x) # graphic of f: [0, 10] to [0, 10], f(x) = x
    plt.plot(x, np.sin(x)) # graphic of f(x) = sin(x)

    x = n.linspace(0, 10, 1000)
    plt.plot(x, np.sin(x ** 2))
    plt.grid()
    plt.xlabel("axis x")
    plt.title("$\sin x\^2$")
\end{lstlisting}


\chapter{Градиентный спуск}
Пусть нам дана функция $f$. Найдем минимум функции.

Величина \textbf{learning rate} -- скорость обучения.
Сами итеррации можно назвать эпохами.

Градиентный спуск может быть стахастическим.
В этом случае градиент можно считать не целиком, выбирая только часть каких-то параметров.

Иногда можно иметь шаг не константный, а подбирать его каждый раз при помощи какого-то метода.

Градиент можно вычислять при помощи численных методов.
Например, через центральную разность

$h = \varepsilon$, $f_k^{\prime} (X) = \dfrac{f(X + h \cdot (0, \dots, 1_k, \dots, 0)) - f(X - h \cdot (0, \dots, 1_k, \dots, 0))}{2h}$.

Еще один важный вопрос -- масштабирование. Удобно искать решения, если расстояня при приближении по ращличным осям примерно равны. Хуже, когда есть вытянутые овраги.

Число обусловенности (condition number).
Абсолютное число обучловенности:
\[
    \lim_{\varepsilon \to 0} \sup_{\|\delta x \| \leqslant \varepsilon} \dfrac{\|\delta f(x)\|}{\| \delta x \|}.
\]


 